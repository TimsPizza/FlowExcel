"""
FlowExcelæµ‹è¯•æ¡†æ¶åŸºç¡€ç±»
æä¾›æµ‹è¯•é…ç½®åŠ è½½ã€ç»“æœéªŒè¯ã€é”™è¯¯å¤„ç†ç­‰æ ¸å¿ƒåŠŸèƒ½
"""

import json
import os
import sys
import unittest
import traceback
from typing import Dict, List, Any, Optional, Union
from pathlib import Path
import pandas as pd

# æ·»åŠ srcåˆ°Pythonè·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '../..'))

from pipeline.models import (
    BaseNode, GlobalContext, PathContext, BranchContext,
    FileInfo, ExecutionMode, NodeType, IndexValue
)
from pipeline.processors import (
    IndexSourceProcessor, SheetSelectorProcessor, RowFilterProcessor,
    RowLookupProcessor, AggregatorProcessor, OutputProcessor
)


class TestResult:
    """æµ‹è¯•ç»“æœç±»"""
    
    def __init__(self, test_id: str, success: bool, error: Optional[str] = None, 
                 actual_result: Any = None, expected_result: Any = None, 
                 execution_time: float = 0.0):
        self.test_id = test_id
        self.success = success
        self.error = error
        self.actual_result = actual_result
        self.expected_result = expected_result
        self.execution_time = execution_time


class TestSuiteResult:
    """æµ‹è¯•å¥—ä»¶ç»“æœç±»"""
    
    def __init__(self, suite_name: str):
        self.suite_name = suite_name
        self.test_results: List[TestResult] = []
        self.total_tests = 0
        self.passed_tests = 0
        self.failed_tests = 0
        self.total_time = 0.0
    
    def add_test_result(self, result: TestResult):
        self.test_results.append(result)
        self.total_tests += 1
        if result.success:
            self.passed_tests += 1
        else:
            self.failed_tests += 1
        self.total_time += result.execution_time
    
    def get_summary(self) -> str:
        success_rate = (self.passed_tests / self.total_tests * 100) if self.total_tests > 0 else 0
        return (f"{self.suite_name}: {self.passed_tests}/{self.total_tests} é€šè¿‡ "
                f"({success_rate:.1f}%), è€—æ—¶: {self.total_time:.2f}s")


class BaseTestFramework:
    """åŸºç¡€æµ‹è¯•æ¡†æ¶ç±»"""
    
    def __init__(self, test_specs_dir: str = "test_data/test_specs"):
        self.test_specs_dir = Path(test_specs_dir)
        self.test_data_dir = Path("test_data")
        self.excel_file_path = self.test_data_dir / "excel_files" / "test_case.xlsx"
        self.global_context = None
        
        # å¤„ç†å™¨æ˜ å°„
        self.processors = {
            NodeType.INDEX_SOURCE: IndexSourceProcessor(),
            NodeType.SHEET_SELECTOR: SheetSelectorProcessor(),
            NodeType.ROW_FILTER: RowFilterProcessor(),
            NodeType.ROW_LOOKUP: RowLookupProcessor(),
            NodeType.AGGREGATOR: AggregatorProcessor(),
            NodeType.OUTPUT: OutputProcessor(),
        }
    
    def setup_global_context(self) -> GlobalContext:
        """è®¾ç½®å…¨å±€ä¸Šä¸‹æ–‡"""
        if self.global_context is not None:
            return self.global_context
            
        # åˆ›å»ºæ–‡ä»¶ä¿¡æ¯
        file_info = FileInfo(
            id="test-file-93fac9a3-f4b3-410a-932c-32620bd11122",
            name="test_case.xlsx",
            path=str(self.excel_file_path),
            sheet_metas=[
                {"sheet_name": "Sheet1_Perfect_Clean", "header_row": 0},
                {"sheet_name": "Sheet2_Slightly_Messy", "header_row": 0},
                {"sheet_name": "Sheet3_Moderately_Complex", "header_row": 0},
                {"sheet_name": "Sheet4_Very_Messy", "header_row": 0},
                {"sheet_name": "Sheet5_Extreme_Chaos", "header_row": 0},
                {"sheet_name": "Sheet6_Edge_Cases", "header_row": 0},
                {"sheet_name": "Sheet7_Real_World_Sales", "header_row": 0},
                {"sheet_name": "Sheet8_Inventory_Complex", "header_row": 0},
            ]
        )
        
        self.global_context = GlobalContext(
            files={"test-file-93fac9a3-f4b3-410a-932c-32620bd11122": file_info},
            execution_mode=ExecutionMode.TEST
        )
        
        return self.global_context
    
    def load_test_specs(self, filename: str) -> Dict[str, Any]:
        """åŠ è½½æµ‹è¯•è§„èŒƒæ–‡ä»¶"""
        spec_path = self.test_specs_dir / filename
        try:
            with open(spec_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            raise FileNotFoundError(f"æ— æ³•åŠ è½½æµ‹è¯•è§„èŒƒæ–‡ä»¶ {spec_path}: {e}")
    
    def create_node_from_config(self, node_config: Dict[str, Any]) -> BaseNode:
        """ä»é…ç½®åˆ›å»ºèŠ‚ç‚¹"""
        return BaseNode(
            id=node_config["id"],
            type=NodeType(node_config["type"]),
            data=node_config["data"]
        )
    
    def validate_result(self, actual: Any, expected: Dict[str, Any], 
                       validation_rules: Dict[str, Any]) -> tuple[bool, str]:
        """éªŒè¯ç»“æœï¼Œè¿”å›(æ˜¯å¦æˆåŠŸ, è¯¦ç»†é”™è¯¯ä¿¡æ¯)"""
        try:
            if expected["type"] == "index_list":
                if not hasattr(actual, 'index_values'):
                    return False, f"æœŸæœ›æœ‰index_valueså±æ€§çš„å¯¹è±¡ï¼Œä½†å¾—åˆ°: {type(actual)}"
                actual_values = [str(val) for val in actual.index_values]
                expected_values = expected["data"]
                
                # æ£€æŸ¥æ•°é‡
                if len(actual_values) != expected["count"]:
                    return False, f"æ•°é‡ä¸åŒ¹é…: æœŸæœ›{expected['count']}ä¸ªå€¼ï¼Œå®é™…å¾—åˆ°{len(actual_values)}ä¸ªå€¼"
                
                # æ£€æŸ¥å”¯ä¸€æ€§
                if validation_rules.get("check_unique", False):
                    if len(set(actual_values)) != len(actual_values):
                        duplicates = [x for x in actual_values if actual_values.count(x) > 1]
                        return False, f"å”¯ä¸€æ€§æ£€æŸ¥å¤±è´¥: å‘ç°é‡å¤å€¼ {list(set(duplicates))}"
                
                # æ£€æŸ¥å†…å®¹ï¼ˆå¿½ç•¥é¡ºåºï¼‰
                actual_set = set(actual_values)
                expected_set = set(expected_values)
                
                if actual_set != expected_set:
                    missing = expected_set - actual_set
                    extra = actual_set - expected_set
                    error_parts = []
                    if missing:
                        error_parts.append(f"ç¼ºå¤±å€¼: {list(missing)}")
                    if extra:
                        error_parts.append(f"å¤šä½™å€¼: {list(extra)}")
                    return False, f"å†…å®¹ä¸åŒ¹é… - {', '.join(error_parts)}"
                
                return True, ""
                
            elif expected["type"] == "dataframe":
                if not hasattr(actual, 'dataframe') or not isinstance(actual.dataframe, pd.DataFrame):
                    return False, f"æœŸæœ›DataFrameå¯¹è±¡ï¼Œä½†å¾—åˆ°: {type(actual)}"
                
                df = actual.dataframe
                
                # éªŒè¯è¡Œæ•°
                if hasattr(expected, 'row_count') and len(df) != expected["row_count"]:
                    return False, f"è¡Œæ•°ä¸åŒ¹é…: æœŸæœ›{expected['row_count']}è¡Œï¼Œå®é™…å¾—åˆ°{len(df)}è¡Œ"
                
                # éªŒè¯å·¥ä½œè¡¨å
                if "sheet_name" in expected:
                    if hasattr(actual, 'sheet_name'):
                        if actual.sheet_name != expected["sheet_name"]:
                            return False, f"å·¥ä½œè¡¨åä¸åŒ¹é…: æœŸæœ›'{expected['sheet_name']}'ï¼Œå®é™…å¾—åˆ°'{actual.sheet_name}'"
                    else:
                        return False, f"æœŸæœ›å·¥ä½œè¡¨å'{expected['sheet_name']}'ï¼Œä½†ç»“æœä¸­æ²¡æœ‰sheet_nameå±æ€§"
                
                return True, ""
                
            elif expected["type"] == "aggregation_result":
                if not hasattr(actual, 'result'):
                    return False, f"æœŸæœ›èšåˆç»“æœå¯¹è±¡ï¼Œä½†å¾—åˆ°: {type(actual)}"
                
                result = actual.result
                expected_method = expected["aggregation_method"]
                expected_value = expected["result_value"]
                
                # æ£€æŸ¥èšåˆæ–¹æ³•
                if str(result.operation) != expected_method:
                    return False, f"èšåˆæ–¹æ³•ä¸åŒ¹é…: æœŸæœ›'{expected_method}'ï¼Œå®é™…å¾—åˆ°'{result.operation}'"
                
                # æ£€æŸ¥æ•°å€¼ç»“æœï¼ˆè€ƒè™‘æµ®ç‚¹ç²¾åº¦ï¼‰
                if isinstance(expected_value, (int, float)) and isinstance(result.result_value, (int, float)):
                    diff = abs(float(result.result_value) - float(expected_value))
                    if diff >= 0.0001:
                        return False, f"æ•°å€¼ç»“æœä¸åŒ¹é…: æœŸæœ›{expected_value}ï¼Œå®é™…å¾—åˆ°{result.result_value}ï¼ˆå·®å¼‚: {diff}ï¼‰"
                else:
                    if str(result.result_value) != str(expected_value):
                        return False, f"ç»“æœå€¼ä¸åŒ¹é…: æœŸæœ›'{expected_value}'ï¼Œå®é™…å¾—åˆ°'{result.result_value}'"
                
                return True, ""
            
            return True, ""
            
        except Exception as e:
            return False, f"éªŒè¯è¿‡ç¨‹ä¸­å‡ºé”™: {type(e).__name__}: {str(e)}"
    
    def run_single_test(self, test_case: Dict[str, Any], processor, 
                       global_context: GlobalContext) -> TestResult:
        """è¿è¡Œå•ä¸ªæµ‹è¯•ç”¨ä¾‹"""
        test_id = test_case["test_id"]
        start_time = pd.Timestamp.now()
        
        try:
            # åˆ›å»ºèŠ‚ç‚¹
            node = self.create_node_from_config(test_case["node_config"])
            
            # åˆ›å»ºè·¯å¾„ä¸Šä¸‹æ–‡
            path_context = PathContext(current_index=IndexValue("test"))
            
            # å‡†å¤‡è¾“å…¥æ•°æ®
            input_data = self._prepare_input_data(test_case, processor.node_type)
            
            # æ‰§è¡ŒèŠ‚ç‚¹å¤„ç†
            actual_result = processor.process(
                node=node,
                input_data=input_data,
                global_context=global_context,
                path_context=path_context
            )
            
            # éªŒè¯ç»“æœ
            validation_rules = test_case.get("validation_rules", {})
            is_valid, error_msg = self.validate_result(
                actual_result, 
                test_case["expected_result"], 
                validation_rules
            )
            
            execution_time = (pd.Timestamp.now() - start_time).total_seconds()
            
            return TestResult(
                test_id=test_id,
                success=is_valid,
                actual_result=actual_result,
                expected_result=test_case["expected_result"],
                execution_time=execution_time,
                error=error_msg if not is_valid else None
            )
            
        except Exception as e:
            execution_time = (pd.Timestamp.now() - start_time).total_seconds()
            error_msg = f"{type(e).__name__}: {str(e)}\n{traceback.format_exc()}"
            
            return TestResult(
                test_id=test_id,
                success=False,
                error=error_msg,
                execution_time=execution_time
            )
    
    def _prepare_input_data(self, test_case: Dict[str, Any], node_type: NodeType):
        """å‡†å¤‡èŠ‚ç‚¹è¾“å…¥æ•°æ®"""
        from pipeline.models import (
            IndexSourceInput, SheetSelectorInput, RowFilterInput, 
            RowLookupInput, AggregatorInput, OutputInput, IndexValue
        )
        
        if node_type == NodeType.INDEX_SOURCE:
            return IndexSourceInput()
        elif node_type == NodeType.SHEET_SELECTOR:
            # ä¸ºSheetSelectorä½¿ç”¨æ­£ç¡®çš„index_valueï¼Œæ ¹æ®æµ‹è¯•ç”¨ä¾‹çš„æ¨¡å¼å†³å®š
            node_data = test_case["node_config"]["data"]
            mode = node_data.get("mode", "auto_by_index")
            
            if mode == "auto_by_index":
                # å¯¹äºè‡ªåŠ¨ç´¢å¼•æ¨¡å¼ï¼Œä½¿ç”¨æµ‹è¯•æœŸæœ›çš„sheetåä½œä¸ºindex_value
                expected_sheet = test_case["expected_result"]["sheet_name"]
                return SheetSelectorInput(index_value=IndexValue(expected_sheet))
            else:
                # å¯¹äºæ‰‹åŠ¨æ¨¡å¼ï¼Œindex_valueä¸é‡è¦ï¼Œä½†ä»éœ€è¦æä¾›
                return SheetSelectorInput(index_value=IndexValue("manual_mode"))
        elif node_type == NodeType.ROW_FILTER:
            # éœ€è¦ä»ä¸Šä¸‹æ–‡åŠ è½½DataFrame
            sheet_name = test_case["sheet_name"]
            df = pd.read_excel(self.excel_file_path, sheet_name=sheet_name, header=0)
            return RowFilterInput(dataframe=df, index_value=IndexValue("test"))
        elif node_type == NodeType.ROW_LOOKUP:
            # éœ€è¦ä»ä¸Šä¸‹æ–‡åŠ è½½DataFrame
            sheet_name = test_case["sheet_name"]
            df = pd.read_excel(self.excel_file_path, sheet_name=sheet_name, header=0)
            return RowLookupInput(dataframe=df, index_value=IndexValue("test"))
        elif node_type == NodeType.AGGREGATOR:
            # éœ€è¦ä»ä¸Šä¸‹æ–‡åŠ è½½DataFrame
            sheet_name = test_case["sheet_name"]
            df = pd.read_excel(self.excel_file_path, sheet_name=sheet_name, header=0)
            return AggregatorInput(dataframe=df, index_value=IndexValue("test"))
        elif node_type == NodeType.OUTPUT:
            # ä¸ºOutputèŠ‚ç‚¹åˆ›å»ºmockçš„èšåˆç»“æœæ•°æ®
            mock_branch_results = {
                "branch_1": {
                    IndexValue("test"): {
                        "count": 10,
                        "sum": 1000.0,
                        "avg": 100.0
                    }
                }
            }
            mock_dataframes = {
                "branch_1": pd.DataFrame({
                    "Column1": [1, 2, 3],
                    "Column2": ["A", "B", "C"]
                })
            }
            return OutputInput(
                branch_aggregated_results=mock_branch_results,
                branch_dataframes=mock_dataframes
            )
        else:
            # å…¶ä»–æœªçŸ¥ç±»å‹
            return None
    
    def print_test_summary(self, suite_results: List[TestSuiteResult]):
        """æ‰“å°æµ‹è¯•æ€»ç»“"""
        print("\n" + "="*80)
        print("FlowExcel æµ‹è¯•æ¡†æ¶æ‰§è¡Œæ€»ç»“")
        print("="*80)
        
        total_tests = 0
        total_passed = 0
        total_failed = 0
        total_time = 0.0
        
        for suite_result in suite_results:
            print(f"\nğŸ“Š {suite_result.get_summary()}")
            
            total_tests += suite_result.total_tests
            total_passed += suite_result.passed_tests
            total_failed += suite_result.failed_tests
            total_time += suite_result.total_time
            
            # æ˜¾ç¤ºå¤±è´¥çš„æµ‹è¯•
            failed_tests = [r for r in suite_result.test_results if not r.success]
            if failed_tests:
                print(f"   âŒ å¤±è´¥çš„æµ‹è¯•:")
                for failed_test in failed_tests:
                    print(f"      - {failed_test.test_id}: {failed_test.error}")
        
        print("\n" + "-"*80)
        success_rate = (total_passed / total_tests * 100) if total_tests > 0 else 0
        print(f"ğŸ¯ æ€»è®¡: {total_passed}/{total_tests} é€šè¿‡ ({success_rate:.1f}%)")
        print(f"â±ï¸  æ€»è€—æ—¶: {total_time:.2f}s")
        
        if total_failed == 0:
            print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡!")
        else:
            print(f"âš ï¸  {total_failed} ä¸ªæµ‹è¯•å¤±è´¥")
        
        print("="*80) 