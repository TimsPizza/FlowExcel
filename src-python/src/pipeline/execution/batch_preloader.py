"""
æ‰¹é‡é¢„åŠ è½½å™¨
å¹¶è¡ŒåŠ è½½å¤šä¸ªExcelæ–‡ä»¶çš„å¤šä¸ªSheetï¼Œå¤§å¹…å‡å°‘IOæ¬¡æ•°å’Œä¸Šä¸‹æ–‡åˆ‡æ¢å¼€é”€
ä¸“ä¸ºæ€§èƒ½ä¼˜åŒ–è®¾è®¡ï¼Œæ”¯æŒè¿›åº¦ç›‘æ§å’Œé”™è¯¯å¤„ç†
"""

import pandas as pd
import time
import os
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Dict, Tuple, Optional
from dataclasses import dataclass

from .file_analyzer import FileBatchInfo
from ..models import GlobalContext
from ..performance.analyzer import get_performance_analyzer


@dataclass
class PreloadResult:
    """é¢„åŠ è½½ç»“æœ"""

    file_id: str
    sheet_name: str
    success: bool
    dataframe: Optional[pd.DataFrame] = None
    error: Optional[str] = None
    load_time_ms: float = 0.0
    rows: int = 0


@dataclass
class BatchPreloadSummary:
    """æ‰¹é‡é¢„åŠ è½½æ‘˜è¦"""

    total_sheets: int
    successful_sheets: int
    failed_sheets: int
    total_time_ms: float
    total_rows: int
    total_files_size_bytes: int
    io_reduction_count: int  # å‡å°‘çš„IOæ¬¡æ•°


class BatchPreloader:
    """æ‰¹é‡é¢„åŠ è½½å™¨ - å¹¶è¡ŒåŠ è½½å¤šä¸ªExcelæ–‡ä»¶"""

    def __init__(self, max_workers: int = 4):
        self.max_workers = max_workers
        self.analyzer = get_performance_analyzer()

    def preload_files(
        self, batch_infos: List[FileBatchInfo], global_context: GlobalContext
    ) -> BatchPreloadSummary:
        """
        æ‰¹é‡é¢„åŠ è½½æ–‡ä»¶

        Args:
            batch_infos: æ–‡ä»¶æ‰¹é‡åŠ è½½ä¿¡æ¯åˆ—è¡¨
            global_context: å…¨å±€ä¸Šä¸‹æ–‡ï¼ˆç”¨äºå­˜å‚¨ç¼“å­˜ï¼‰

        Returns:
            æ‰¹é‡é¢„åŠ è½½æ‘˜è¦
        """
        start_time = time.time()

        print(f"PERF: Starting batch preload for {len(batch_infos)} files...")

        # å‡†å¤‡æ‰€æœ‰éœ€è¦åŠ è½½çš„ä»»åŠ¡
        load_tasks = []
        for batch_info in batch_infos:
            for sheet_name in batch_info.required_sheets:
                load_tasks.append((batch_info, sheet_name))

        # è®¡ç®—ä¼°ç®—çš„IOå‡å°‘é‡
        io_reduction = len(load_tasks) - len(batch_infos)

        print(
            f"PERF: Will load {len(load_tasks)} sheets from {len(batch_infos)} files (IO reduction: {io_reduction})"
        )

        # å¹¶è¡Œæ‰§è¡ŒåŠ è½½ä»»åŠ¡
        results = self._execute_parallel_loads(load_tasks, global_context)

        # ç”Ÿæˆæ‘˜è¦
        total_time = (time.time() - start_time) * 1000
        summary = self._create_summary(results, total_time, io_reduction)

        # æ‰“å°æ‘˜è¦å·²åœ¨analyzerä¸­åŒ…å«
        # self._print_summary(summary)

        # è®°å½•æ‰¹é‡é¢„åŠ è½½ç»Ÿè®¡ï¼ˆåˆ†å¼€è®¡ç®—ï¼Œé¿å…ä¸å¸¸è§„Excel IOæ··æ·†ï¼‰
        self.analyzer.onBatchPreloadComplete(
            total_files=len(batch_infos),
            total_sheets=summary.total_sheets,
            successful_sheets=summary.successful_sheets,
            failed_sheets=summary.failed_sheets,
            total_time_ms=summary.total_time_ms,
            total_rows=summary.total_rows,
            total_io_reduction=io_reduction
        )

        return summary

    def _execute_parallel_loads(
        self, load_tasks: List[Tuple[FileBatchInfo, str]], global_context: GlobalContext
    ) -> List[PreloadResult]:
        """
        å¹¶è¡Œæ‰§è¡ŒåŠ è½½ä»»åŠ¡

        Args:
            load_tasks: åŠ è½½ä»»åŠ¡åˆ—è¡¨
            global_context: å…¨å±€ä¸Šä¸‹æ–‡

        Returns:
            é¢„åŠ è½½ç»“æœåˆ—è¡¨
        """
        results = []

        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_task = {
                executor.submit(self._load_single_sheet, batch_info, sheet_name): (
                    batch_info,
                    sheet_name,
                )
                for batch_info, sheet_name in load_tasks
            }

            # æ”¶é›†ç»“æœ
            for future in as_completed(future_to_task):
                batch_info, sheet_name = future_to_task[future]

                try:
                    result = future.result()
                    results.append(result)

                    # å¦‚æœæˆåŠŸï¼Œæ·»åŠ åˆ°ç¼“å­˜
                    if result.success and result.dataframe is not None:
                        cache_key = f"{batch_info.file_id}_{sheet_name}"
                        global_context.loaded_dataframes[cache_key] = result.dataframe

                except Exception as e:
                    # å¤„ç†æ„å¤–é”™è¯¯
                    error_result = PreloadResult(
                        file_id=batch_info.file_id,
                        sheet_name=sheet_name,
                        success=False,
                        error=f"Unexpected error: {str(e)}",
                    )
                    results.append(error_result)

        return results

    def _load_single_sheet(
        self, batch_info: FileBatchInfo, sheet_name: str
    ) -> PreloadResult:
        """
        åŠ è½½å•ä¸ªSheet

        Args:
            batch_info: æ–‡ä»¶æ‰¹é‡ä¿¡æ¯
            sheet_name: Sheetåç§°

        Returns:
            é¢„åŠ è½½ç»“æœ
        """
        start_time = time.time()

        # æ³¨æ„ï¼šæ‰¹é‡é¢„åŠ è½½ä¸è®¡å…¥å¸¸è§„Excel IOç»Ÿè®¡ï¼Œé¿å…é‡å¤è®¡æ•°
        # read_id = self.analyzer.onExcelReadStart(batch_info.file_path, sheet_name)

        try:
            # è·å–header row
            header_row = batch_info.sheet_header_rows.get(sheet_name, 0)

            # è¯»å–Excel
            df = pd.read_excel(
                batch_info.file_path, sheet_name=sheet_name, header=header_row
            )

            load_time_ms = (time.time() - start_time) * 1000

            # è·å–æ–‡ä»¶å¤§å°
            try:
                file_size = os.path.getsize(batch_info.file_path)
            except:
                file_size = None

            # æ‰¹é‡é¢„åŠ è½½ä¸è®¡å…¥å¸¸è§„Excel IOç»Ÿè®¡
            # self.analyzer.onExcelReadFinish(read_id, len(df), file_size)

            return PreloadResult(
                file_id=batch_info.file_id,
                sheet_name=sheet_name,
                success=True,
                dataframe=df,
                load_time_ms=load_time_ms,
                rows=len(df),
            )

        except Exception as e:
            load_time_ms = (time.time() - start_time) * 1000

            # æ‰¹é‡é¢„åŠ è½½ä¸è®¡å…¥å¸¸è§„Excel IOç»Ÿè®¡
            # self.analyzer.onExcelReadFinish(read_id, 0, None)

            return PreloadResult(
                file_id=batch_info.file_id,
                sheet_name=sheet_name,
                success=False,
                error=str(e),
                load_time_ms=load_time_ms,
            )

    def _create_summary(
        self,
        results: List[PreloadResult],
        total_time_ms: float,
        io_reduction_count: int,
    ) -> BatchPreloadSummary:
        """
        åˆ›å»ºæ‰¹é‡é¢„åŠ è½½æ‘˜è¦

        Args:
            results: é¢„åŠ è½½ç»“æœåˆ—è¡¨
            total_time_ms: æ€»æ—¶é—´
            io_reduction_count: IOå‡å°‘æ•°é‡

        Returns:
            æ‰¹é‡é¢„åŠ è½½æ‘˜è¦
        """
        successful_results = [r for r in results if r.success]
        failed_results = [r for r in results if not r.success]

        total_rows = sum(r.rows for r in successful_results)

        # ä¼°ç®—æ–‡ä»¶å¤§å°ï¼ˆåŸºäºæˆåŠŸåŠ è½½çš„æ•°æ®ï¼‰
        estimated_size = total_rows * 50  # ç²—ç•¥ä¼°ç®—æ¯è¡Œ50å­—èŠ‚

        return BatchPreloadSummary(
            total_sheets=len(results),
            successful_sheets=len(successful_results),
            failed_sheets=len(failed_results),
            total_time_ms=total_time_ms,
            total_rows=total_rows,
            total_files_size_bytes=estimated_size,
            io_reduction_count=io_reduction_count,
        )

    def _print_summary(self, summary: BatchPreloadSummary):
        """
        æ‰“å°é¢„åŠ è½½æ‘˜è¦

        Args:
            summary: æ‰¹é‡é¢„åŠ è½½æ‘˜è¦
        """
        print("=" * 50)
        print("ğŸ“¦ æ‰¹é‡é¢„åŠ è½½æŠ¥å‘Š")
        print("=" * 50)
        print(f"ğŸ“Š æ€»Sheetæ•°: {summary.total_sheets}")
        print(f"âœ… æˆåŠŸåŠ è½½: {summary.successful_sheets}")
        print(f"âŒ åŠ è½½å¤±è´¥: {summary.failed_sheets}")
        print(f"â±ï¸ æ€»åŠ è½½æ—¶é—´: {summary.total_time_ms:.2f}ms")
        print(f"ğŸ“‹ æ€»è¡Œæ•°: {summary.total_rows}")
        print(f"ğŸ’¾ ä¼°ç®—å¤§å°: {summary.total_files_size_bytes} bytes")
        print(f"ğŸš€ å‡å°‘IOæ¬¡æ•°: {summary.io_reduction_count}")

        if summary.total_sheets > 0:
            success_rate = summary.successful_sheets / summary.total_sheets * 100
            avg_time = summary.total_time_ms / summary.total_sheets
            print(f"ğŸ“ˆ æˆåŠŸç‡: {success_rate:.1f}%")
            print(f"ğŸ“Š å¹³å‡åŠ è½½æ—¶é—´: {avg_time:.2f}ms/sheet")

        print("=" * 50)
